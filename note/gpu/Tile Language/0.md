é“¾æ¥ï¼š[tile-ai/tilelang | DeepWiki](https://deepwiki.com/tile-ai/tilelang)

# docsï¼šTileLang

![image-20251013174648404](image-20251013174648404.png)

**TileLang**æ˜¯ä¸€ç§*Pythoné£æ ¼çš„é¢†åŸŸç‰¹å®šè¯­è¨€*ï¼Œæ—¨åœ¨ç®€åŒ–*é«˜æ€§èƒ½GPU/CPUå†…æ ¸*çš„åˆ›å»ºã€‚å®ƒä¸ºç”¨æˆ·`æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ¥å£æ¥æè¿°è®¡ç®—è¿‡ç¨‹`ï¼Œè¿™äº›è®¡ç®—éšåé€šè¿‡åŸºäºTVMçš„*é«˜çº§ä¼˜åŒ–æµæ°´çº¿*è¿›è¡Œ*å³æ—¶ç¼–è¯‘ï¼ˆJITï¼‰*ï¼Œä»è€Œä¸ºçŸ©é˜µä¹˜æ³•å’Œæ³¨æ„åŠ›æœºåˆ¶ç­‰AIå·¥ä½œè´Ÿè½½æä¾›æœ€`å…ˆè¿›çš„æ€§èƒ½`ã€‚

## å¯è§†åŒ–

```mermaid
flowchart TD
    A0["TileLangè¯­è¨€æ¥å£ï¼ˆT.Namespaceï¼‰"]
    A1["JITå†…æ ¸ç¼–è¯‘ï¼ˆJITKernelï¼‰"]
    A2["TIRè½¬æ¢æµæ°´çº¿ï¼ˆPassesï¼‰"]
    A3["å¼ é‡æ ¸å¿ƒæ“ä½œï¼ˆGEMM/WGMMAï¼‰"]
    A4["å¸ƒå±€ä¸ç‰‡æ®µç®¡ç†"]
    A5["è‡ªåŠ¨è°ƒä¼˜å™¨"]
    A0 -- "ç¼–è¯‘ä¸º" --> A1
    A0 -- "è¡¨è¾¾" --> A3
    A0 -- "æŒ‡å®š" --> A4
    A1 -- "åº”ç”¨è½¬æ¢" --> A2
    A5 -- "è°ƒä¼˜" --> A1
    A2 -- "é™çº§ä¸º" --> A3
    A2 -- "æ¨æ–­" --> A4
    A5 -- "é…ç½®" --> A2
    A3 -- "éœ€è¦" --> A4
```

## ç« èŠ‚

1. [TileLangè¯­è¨€æ¥å£ï¼ˆT.Namespaceï¼‰](01_tilelang_language_interface__t_namespace__.md)
2. [å¼ é‡æ ¸å¿ƒæ“ä½œï¼ˆGEMM/WGMMAï¼‰](02_tensor_core_operations__gemm___wgmma__.md)
3. [å¸ƒå±€ä¸ç‰‡æ®µç®¡ç†](03_layout_and_fragment_management_.md)
4. [JITå†…æ ¸ç¼–è¯‘ï¼ˆJITKernelï¼‰](04_jit_kernel_compilation__jitkernel__.md)
5. [TIRè½¬æ¢æµæ°´çº¿ï¼ˆPassesï¼‰](05_tir_transformation_pipeline__passes__.md)
6. [è‡ªåŠ¨è°ƒä¼˜å™¨](06_auto_tuner_.md)

---

# tile-lang

** (tile-lang)** æ˜¯ä¸€ç§ç®€æ´çš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼Œæ—¨åœ¨==ç®€åŒ–é«˜æ€§èƒ½GPU/CPUå†…æ ¸(å¦‚GEMMã€åé‡åŒ–GEMMã€FlashAttentionã€LinearAttention)çš„å¼€å‘==

é€šè¿‡åœ¨TVMåŸºç¡€ä¸Šé‡‡ç”¨Pythoné£æ ¼çš„è¯­æ³•å’Œåº•å±‚ç¼–è¯‘å™¨åŸºç¡€è®¾æ–½ï¼Œtile-langè®©å¼€å‘è€…èƒ½å¤Ÿ`ä¸“æ³¨äºç”Ÿäº§åŠ›ï¼ŒåŒæ—¶ä¸ç‰ºç‰²å®ç°æœ€å…ˆè¿›æ€§èƒ½æ‰€éœ€çš„åº•å±‚ä¼˜åŒ–`ã€‚

![MatmulExample.png](https://github.com/tile-ai/tilelang/blob/main/images/MatmulExample.png?raw=true)

## åŠ¨æ€
* 2025/10/07 ğŸ: æ–°å¢Apple Metalè®¾å¤‡æ”¯æŒï¼Œè¯¦æƒ…æŸ¥çœ‹Pull Request #799
* 2025/09/29 ğŸ‰: æ¿€åŠ¨å®£å¸ƒç°å·²æ”¯æŒé¢å‘åä¸ºæ˜‡è…¾èŠ¯ç‰‡çš„**AscendC**å’Œ**AscendNPU IR**åç«¯ï¼é¢„è§ˆé“¾æ¥ï¼šğŸ”— è¯¦è§ascendc_ptoå’Œnpuirä¸¤ä¸ªåˆ†æ”¯çš„å®ç°
* 2025/07/04 ğŸš€: å¼•å…¥T.gemm_spæ”¯æŒ2:4ç¨€ç–å¼ é‡æ ¸å¿ƒï¼Œè¯¦æƒ…æŸ¥çœ‹Pull Request #526
* 2025/06/05 âœ¨: æ–°å¢NVRTCåç«¯æ˜¾è‘—å‡å°‘cuteæ¨¡æ¿ç¼–è¯‘æ—¶é—´
* 2025/04/14 ğŸš€: ä¸ºAMD MI300Xæ–°å¢é«˜æ€§èƒ½FlashMLAå®ç°ï¼Œæ€§èƒ½åª²ç¾Aiteræ‰‹å·¥ä¼˜åŒ–æ±‡ç¼–å†…æ ¸(è§example_mla_amd)
* 2025/03/03 ğŸš€: ä»…ç”¨80è¡ŒPythonä»£ç å®ç°é«˜æ€§èƒ½MLAè§£ç ï¼Œåœ¨H100ä¸Šè¾¾åˆ°ä¸FlashMLAç›¸å½“çš„æ€§èƒ½(è§example_mla_decode.py)
* 2025/02/15 âœ¨: æ–°å¢WebGPUä»£ç ç”Ÿæˆæ”¯æŒ(è§Pull Request #86)
* 2025/02/12 âœ¨: æ¿€åŠ¨å‘å¸ƒv0.1.0ç‰ˆæœ¬
* 2025/02/10 ğŸš€: æ–°å¢è°ƒè¯•å·¥å…·T.print(æ‰“å°å˜é‡/ç¼“å†²åŒº)å’Œå†…å­˜å¸ƒå±€ç»˜å›¾å™¨(examples/plot_layout)
* 2025/01/20 âœ¨: é«˜æ€§èƒ½AIå·¥ä½œè´Ÿè½½DSL tile-langæ­£å¼å¼€æº

## æµ‹è¯•è®¾å¤‡
è™½ç„¶tile-langæ—¨åœ¨è·¨å¤šç§è®¾å¤‡é€šç”¨ï¼Œä½†å·²åœ¨ä»¥ä¸‹è®¾å¤‡ä¸Šä¸“é—¨æµ‹è¯•éªŒè¯ï¼š
- NVIDIA GPU: H100(æ”¯æŒAuto TMA/WGMMA)ã€A100ã€V100ã€RTX 4090/3090/A6000
- AMD GPU: MI250(æ”¯æŒAuto MatrixCore)ã€MI300X(æ”¯æŒAsync Copy)

## ç®—å­å®ç°ç¤ºä¾‹
tile-langä¸ºå®ç°å„ç±»ç®—å­æä¾›äº†åŸºç¡€æ„å»ºæ¨¡å—ï¼ŒåŒ…æ‹¬ï¼š
- çŸ©é˜µä¹˜æ³•
- åé‡åŒ–GEMM  
- FlashAttention
- Flash LinearAttention
- Flash MLAè§£ç 
- åŸç”Ÿç¨€ç–æ³¨æ„åŠ›

åœ¨examplesç›®å½•ä¸­è¿˜å¯æ‰¾åˆ°æ›´å¤šå¤æ‚å†…æ ¸å®ç°ï¼Œå¦‚å·ç§¯ã€FlashAttentionå‰å‘/åå‘ä¼ æ’­ç­‰ï¼Œæ›´å¤šç®—å­å°†æŒç»­æ·»åŠ ã€‚

## åŸºå‡†æµ‹è¯•æ‘˜è¦
tile-langåœ¨å„ç§è®¡ç®—æ¨¡å¼ä¸­å‡è¡¨ç°å‡ºå“è¶Šæ€§èƒ½ã€‚å®Œæ•´æµ‹è¯•è„šæœ¬å’Œè®¾ç½®è¯¦è§[tilelang-benchmark](é“¾æ¥)ï¼Œéƒ¨åˆ†äº®ç‚¹ç»“æœï¼š

### H100ä¸Šçš„MLAè§£ç æ€§èƒ½
![image-20251013174919301](image-20251013174919301.png)

### H100ä¸Šçš„FlashAttentionæ€§èƒ½

![operator performance on H100](https://github.com/tile-ai/tilelang/raw/main/images/mha_performance_h100.png)

### å¤šGPUçŸ©é˜µä¹˜æ€§èƒ½(RTX 4090/A100/H100/MI300X)
![gemm fp16 performance on Gpus](https://github.com/tile-ai/tilelang/raw/main/images/op_benchmark_consistent_gemm_fp16.png)

### A100ä¸Šçš„åé‡åŒ–çŸ©é˜µä¹˜æ€§èƒ½
![dequantize gemv performance on A100](https://github.com/tile-ai/tilelang/raw/main/images/op_benchmark_a100_wq_gemv.png)

## å®‰è£…æ–¹å¼

### æ–¹æ³•1: Pipå®‰è£…
ä»PyPIå®‰è£…æœ€æ–°å‘å¸ƒç‰ˆï¼š
```bash
pip install tilelang
```

æˆ–ç›´æ¥ä»GitHubä»“åº“å®‰è£…ï¼š
```bash
pip install git+https://github.com/tile-ai/tilelang
```

æœ¬åœ°å®‰è£…ï¼š
```bash
sudo apt-get update
sudo apt-get install -y python3-setuptools gcc libtinfo-dev zlib1g-dev build-essential cmake libedit-dev libxml2-dev
pip install -e . -v  # ç§»é™¤-eé€‰é¡¹å¯éå¼€å‘æ¨¡å¼å®‰è£…ï¼Œ-væ˜¾ç¤ºè¯¦ç»†è¾“å‡º
```

### æ–¹æ³•2: ä»æºç æ„å»º
æä¾›ä¸‰ç§æºç å®‰è£…æ–¹å¼ï¼š
1. ä½¿ç”¨è‡ªæœ‰TVMå®‰è£…
2. ä½¿ç”¨æ†ç»‘çš„TVMå­æ¨¡å—  
3. ä½¿ç”¨æä¾›çš„è„šæœ¬å®‰è£…

### æ–¹æ³•3: å®‰è£…Nightlyç‰ˆæœ¬
```bash
pip install tilelang -f https://tile-ai.github.io/whl/nightly/cu121/
```
æ³¨æ„ï¼šNightlyç‰ˆæœ¬åŒ…å«æœ€æ–°ä»£ç å˜æ›´ä½†ç¨³å®šæ€§å¯èƒ½ä½äºæ­£å¼ç‰ˆï¼Œé€‚åˆæµ‹è¯•æ–°ç‰¹æ€§æˆ–æ€¥éœ€æœªå‘å¸ƒçš„é—®é¢˜ä¿®å¤ã€‚

## å¿«é€Ÿå…¥é—¨
æœ¬èŠ‚å±•ç¤ºå¦‚ä½•ä½¿ç”¨tile-langç¼–å†™å’Œæ‰§è¡Œç®€å•çš„GEMM(çŸ©é˜µä¹˜æ³•)å†…æ ¸ï¼Œä»¥åŠå¸ƒå±€ä¼˜åŒ–ã€æµæ°´çº¿å’ŒL2ç¼“å­˜å‹å¥½å‹swizzlingç­‰æŠ€æœ¯ã€‚

### å¸¦æ³¨è§£çš„GEMMç¤ºä¾‹(å¸ƒå±€ã€L2ç¼“å­˜Swizzlingå’Œæµæ°´çº¿ç­‰)
```python
import tilelang
import tilelang.language as T

@tilelang.jit
def matmul(M, N, K, block_M, block_N, block_K, dtype="float16", accum_dtype="float"):
    
    @T.prim_func
    def matmul_relu_kernel(
            A: T.Tensor((M, K), dtype),
            B: T.Tensor((K, N), dtype),
            C: T.Tensor((M, N), dtype),
    ):
        with T.Kernel(T.ceildiv(N, block_N), T.ceildiv(M, block_M), threads=128) as (bx, by):
            A_shared = T.alloc_shared((block_M, block_K), dtype)
            B_shared = T.alloc_shared((block_K, block_N), dtype)
            C_local = T.alloc_fragment((block_M, block_N), accum_dtype)

            T.clear(C_local)

            for ko in T.Pipelined(T.ceildiv(K, block_K), num_stages=3):
                T.copy(A[by * block_M, ko * block_K], A_shared)
                T.copy(B[ko * block_K, bx * block_N], B_shared)
                T.gemm(A_shared, B_shared, C_local)
            
            for i, j in T.Parallel(block_M, block_N):
                C_local[i, j] = T.max(C_local[i, j], 0)

            T.copy(C_local, C[by * block_M, bx * block_N])

    return matmul_relu_kernel

# æµ‹è¯•ä»£ç 
M = N = K = 1024
block_M = block_N = 128
block_K = 32
matmul_relu_kernel = matmul(M, N, K, block_M, block_N, block_K)

import torch
a = torch.randn(M, K, device="cuda", dtype=torch.float16)
b = torch.randn(K, N, device="cuda", dtype=torch.float16)  
c = torch.empty(M, N, device="cuda", dtype=torch.float16)

matmul_relu_kernel(a, b, c)
ref_c = torch.relu(a @ b)
torch.testing.assert_close(c, ref_c, rtol=1e-2, atol=1e-2)
print("éªŒè¯é€šè¿‡")

# æ€§èƒ½åˆ†æ
profiler = matmul_relu_kernel.get_profiler(tensor_supply_type=tilelang.TensorSupplyType.Normal)
print(f"å»¶è¿Ÿ: {profiler.do_bench()} ms")
```

## è¶…è¶ŠGEMMçš„æ·±åº¦æ¢ç´¢
é™¤GEMMå¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›å¤šç§ç¤ºä¾‹å±•ç¤ºTileLangçš„å¤šåŠŸèƒ½æ€§å’Œå¼ºå¤§èƒ½åŠ›ï¼š
- **åé‡åŒ–GEMM**ï¼šé€šè¿‡ç²¾ç»†æ§åˆ¶æ¯çº¿ç¨‹æ“ä½œå®ç°é«˜æ€§èƒ½åé‡åŒ–ï¼Œå¤šé¡¹ç‰¹æ€§å·²è¢«BitBLASé‡‡çº³ä¸ºé»˜è®¤è¡Œä¸º
- **FlashAttention**ï¼šç”¨ç®€æ´ç›´è§‚çš„è¯­æ³•å®ç°è·¨ç®—å­èåˆï¼Œå¹¶æä¾›è‡ªåŠ¨è°ƒä¼˜ç¤ºä¾‹
- **çº¿æ€§æ³¨æ„åŠ›**ï¼šåŒ…æ‹¬RetNetå’ŒMambaå®ç°
- **å·ç§¯**ï¼šå®ç°IM2Colå·ç§¯

## å³å°†æ¨å‡ºçš„ç‰¹æ€§
æŸ¥çœ‹[tilelang v0.2.0å‘å¸ƒè®¡åˆ’](é“¾æ¥)äº†è§£å³å°†æ¨å‡ºçš„åŠŸèƒ½ã€‚TileLangç°å·²åœ¨BitBLASå’ŒAttentionEngineé¡¹ç›®ä¸­ä½¿ç”¨ã€‚

-----------

![image-20251013174541941](image-20251013174541941.png)

å¯ä»¥é€šè¿‡è¯¢é—®devinæ¥äº†è§£å­¦ä¹ ~
