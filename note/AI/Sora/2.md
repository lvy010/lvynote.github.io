# 第2章：扩散模型

欢迎回来

在[第1章：文本调节器](01_text_conditioners_.md)中，我们了解了Open-Sora如何将"日出时分无人机拍摄的雾霭笼罩山间湖泊的宁静画面"这类创意构想转化为计算机能理解的数字指令（嵌入向量）。这些指令就像是给视频生成"艺术家"的详细剧本和情绪板。

现在想象我们已经有了这些数字指令。计算机如何真正*创造*出视频？

> 如何从抽象数字变成美丽动态的画面？

这就是**扩散模型**的舞台！它是Open-Sora的核心"艺术家"，==负责接收数字指令，从完全混沌的状态开始，逐步雕琢出你想象中的连贯视频==。

## 扩散模型的工作原理

扩散模型的概念非常巧妙，就像艺术家从模糊嘈杂的图像开始，逐步使其变得清晰细致。

核心流程如下：

1. **始于随机噪声**：扩散模型不是从空白画布开始，而是从充满纯静态或随机噪声的"画布"起步，就像老式电视的雪花屏
2. **渐进去噪**：通过许多小步骤，每次尝试去除少量噪声，使图像或视频略微清晰
3. **指令引导**：每个步骤都使用[文本调节器](01_text_conditioners_.md)生成的数字编码作为引导，确保去噪过程朝着预期画面发展
4. **精炼成连贯视频**：经过多次去噪后，随机噪声转变为可识别的高质量视频序列

这就像雕塑家从无形的黏土块开始，通过每次精心雕琢，最终呈现出细节丰富的雕像，整个过程都遵循着设计蓝图。

## Open-Sora中的扩散模型实现

Open-Sora使用==基于**Transformer**的神经网络==构建扩散模型。这种架构擅长理解数据关系，无论是句子中的词语还是视频中的像素/帧。Open-Sora的Transformer专门设计用于处理**空间**（如图像细节）和**时间**（如视频中的运动变化）信息。

文本提示转化为视频的总体流程：

```mermaid
sequenceDiagram
    participant 用户
    participant 准备函数
    participant 扩散模型
    participant 视频自动编码器(VAE)

    用户->>准备函数: "雪峰上空翱翔的威严巨龙"
    备注在准备函数上: 获取T5和CLIP嵌入
    准备函数->>扩散模型: 初始噪声+文本嵌入+时间步长
    备注在扩散模型上: 迭代预测并去除噪声
    扩散模型-->>扩散模型: 精炼含噪视频(多步骤)
    扩散模型->>视频自动编码器(VAE): 去噪后的视频潜在表示
    备注在视频自动编码器(VAE)上: 将压缩数据转为实际视频帧
    视频自动编码器(VAE)-->>用户: 生成的视频
```

### 代码

Open-Sora的推理脚本(`scripts/diffusion/inference.py`)通过`api_fn`封装整个视频生成过程：

```python
# api_fn是调用扩散模型的辅助函数
x = api_fn(
    sampling_option, # 帧数、分辨率等设置
    cond_type,       # 条件类型(如"t2v"表示文生视频)
    seed=None,       # 随机种子确保可复现性
    patch_size=2,    # 视频处理的内部细节
    channel=4,       # 视频数据的通道数(潜在空间)
    **batch,         # 包含第1章的文本嵌入
).cpu()
```

当调用`api_fn`时，扩散模型开始工作。它接收文本嵌入(`batch`)、视频特性设置(`sampling_option`)和随机噪声起点，通过迭代将噪声转化为最终视频

输出`x`是生成的视频数据（仍为压缩格式，需通过[视频自动编码器(VAE)](03_video_autoencoder__vae__.md)解压）。

## 艺术工作室内部：MMDiT模型

Open-Sora的扩散模型具体实现为`MMDiTModel`（多模态扩散Transformer），其定义见`opensora/models/mmdit/model.py`。

### 1. 准备画布与指令

在`MMDiTModel`开始创作前，需要通过`prepare_inputs`函数（位于`scripts/diffusion/train.py`）准备输入：

```python
def prepare_inputs(batch):
    x_0 = batch.pop("video_latents") # 来自VAE的压缩视频数据
    y = batch.pop("text")            # 文本提示
    
    # 生成随机噪声x_1，与x_0混合得到含噪输入x_t
    x_1 = torch.randn_like(x_0)
    t = torch.sigmoid(torch.randn((bs), device=device)) # 0到1之间的随机时间步
    x_t = 混合公式(x_0, x_1, t)

    # 获取文本嵌入
    t5_embedding = model_t5(y)
    clip_embedding = model_clip(y)

    # 打包扩散模型输入
    inp = {
        "img": x_t,       # 含噪视频潜在表示
        "timesteps": t,   # 噪声程度(指导去噪过程)
        "txt": t5_embedding, # 第1章的T5嵌入
        "y_vec": clip_embedding, # 第1章的CLIP嵌入
    }
    return inp
```

`MMDiTModel`的`prepare_block_inputs`方法进一步处理这些输入：

```python
def prepare_block_inputs(self, img, img_ids, txt, txt_ids, timesteps, y_vec, **kwargs):
    # 1. 将初始视频数据投影到模型的隐藏空间
    img = self.img_in(img)
    
    # 2. 嵌入时间步和CLIP向量
    vec = self.time_in(timestep_embedding(timesteps, 256))
    vec = vec + self.vector_in(y_vec)
    
    # 3. 投影T5文本嵌入
    txt = self.txt_in(txt)
    
    # 4. 创建视频和文本的位置嵌入
    ids = torch.cat((txt_ids, img_ids), dim=1)
    pe = self.pe_embedder(ids)
    
    return img, txt, vec, pe  # Transformer块的主输入
```

### 2. 核心去噪过程

实际的"艺术创作"发生在`MMDiTModel`的`forward`方法中，通过一系列Transformer块精炼含噪视频数据：

```python
def forward(self, img, img_ids, txt, txt_ids, timesteps, y_vec, **kwargs):
    # 准备输入
    img, txt, vec, pe = self.prepare_block_inputs(...)
    
    # 双重块处理(同时精炼视频和文本)
    for block in self.double_blocks:
        img, txt = block(img, txt, vec, pe)
    
    # 单一块处理(组合特征)
    img = torch.cat((txt, img), 1)
    for block in self.single_blocks:
        img = block(img, vec, pe)
    
    # 最终输出处理
    img = img[:, txt.shape[1]:, ...]
    img = self.final_layer(img, vec)
    return img  # 预测的噪声或原始视频
```

在这些`double_blocks`和`single_blocks`循环中，复杂的自注意力和前馈神经网络（Transformer核心）发挥作用。它们观察含噪视频的所有部分、文本指令和当前噪声水平(`vec`和`pe`)，以最有效的方式去除噪声。

每个`block`还使用`Modulation`层（定义于`opensora/models/mmdit/layers.py`），这是将`vec`（时间和CLIP嵌入）注入网络的关键部分，确保去噪过程始终受文本提示和当前扩散阶段引导。

最终`final_layer`将精炼的内部表示转换为预期输出——可能是需要减去的*预测噪声*，或是*原始清晰视频*的直接预测（取决于具体扩散公式）

- 这个预测结果用于向清晰视频迈进一步，该过程重复多次直至视频清晰。

## 总结

扩散模型是Open-Sora的核心"艺术家"，它==巧妙地将随机噪声转化为连贯的高质量视频序列==，整个过程由[文本调节器](01_text_conditioners_.md)提供的精确数字指令引导。

通过基于专用`Transformer架构`的迭代去噪过程，它将你的构想转化为视觉现实。

现在我们了解了扩散模型如何创建这些初始的压缩视频表示，接下来让我们探索Open-Sora如何将这些==压缩数据扩展成完整的可观看视频帧==。这正是[视频自动编码器(VAE)](03_video_autoencoder__vae__.md)的职责所在！

